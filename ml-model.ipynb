{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:24:36.018702Z",
          "iopub.status.busy": "2022-08-31T02:24:36.018343Z",
          "iopub.status.idle": "2022-08-31T02:24:41.302368Z",
          "shell.execute_reply": "2022-08-31T02:24:41.301332Z",
          "shell.execute_reply.started": "2022-08-31T02:24:36.018623Z"
        },
        "id": "AldVDvOgcpbc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from math import sqrt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T02:26:42.334483Z",
          "iopub.status.busy": "2022-08-31T02:26:42.334127Z",
          "iopub.status.idle": "2022-08-31T02:26:46.721553Z",
          "shell.execute_reply": "2022-08-31T02:26:46.720477Z",
          "shell.execute_reply.started": "2022-08-31T02:26:42.334452Z"
        },
        "id": "6xHq8kvDw8bX",
        "outputId": "df75877b-52b2-49c3-d697-5ce518c1b755",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''!wget http://images.cocodataset.org/zips/train2017.zip'''\n",
        "\n",
        "\n",
        "\n",
        "with open(f'{BASE_PATH}/annotations/captions_train2017.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    data = data['annotations']\n",
        "\n",
        "img_cap_pairs = []\n",
        "\n",
        "for sample in data:\n",
        "    img_name = '%012d.jpg' % sample['image_id']\n",
        "    img_cap_pairs.append([img_name, sample['caption']])\n",
        "\n",
        "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
        "captions['image'] = captions['image'].apply(\n",
        "    lambda x: f'{BASE_PATH}/train2017/{x}'\n",
        ")\n",
        "captions = captions.sample(70000)\n",
        "captions = captions.reset_index(drop=True)\n",
        "captions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:26:41.770673Z",
          "iopub.status.busy": "2022-08-31T02:26:41.769995Z",
          "iopub.status.idle": "2022-08-31T02:26:41.776645Z",
          "shell.execute_reply": "2022-08-31T02:26:41.775655Z",
          "shell.execute_reply.started": "2022-08-31T02:26:41.770632Z"
        },
        "id": "RLqCnQPRw8bW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '../input/coco-2017-dataset/coco2017'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:26:46.724375Z",
          "iopub.status.busy": "2022-08-31T02:26:46.723726Z",
          "iopub.status.idle": "2022-08-31T02:26:46.730435Z",
          "shell.execute_reply": "2022-08-31T02:26:46.729484Z",
          "shell.execute_reply.started": "2022-08-31T02:26:46.724337Z"
        },
        "id": "rWbe_xuhFaJp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = '[start] ' + text + ' [end]'\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T02:26:46.732378Z",
          "iopub.status.busy": "2022-08-31T02:26:46.732026Z",
          "iopub.status.idle": "2022-08-31T02:26:47.198330Z",
          "shell.execute_reply": "2022-08-31T02:26:47.197436Z",
          "shell.execute_reply.started": "2022-08-31T02:26:46.732341Z"
        },
        "id": "v_ouwWhKnEy5",
        "outputId": "540d6b75-b7ab-4fb5-a88e-ab61c264d8ee",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "captions['caption'] = captions['caption'].apply(preprocess)\n",
        "captions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T02:26:47.201894Z",
          "iopub.status.busy": "2022-08-31T02:26:47.201583Z",
          "iopub.status.idle": "2022-08-31T02:26:47.307308Z",
          "shell.execute_reply": "2022-08-31T02:26:47.306439Z",
          "shell.execute_reply.started": "2022-08-31T02:26:47.201866Z"
        },
        "id": "6RBuExHWnGEt",
        "outputId": "58ce3822-1046-47db-8038-3bf8afc2b518",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "random_row = captions.sample(1).iloc[0]\n",
        "print(random_row.caption)\n",
        "print()\n",
        "im = Image.open(random_row.image)\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:02.627478Z",
          "iopub.status.busy": "2022-08-31T02:27:02.627088Z",
          "iopub.status.idle": "2022-08-31T02:27:02.633098Z",
          "shell.execute_reply": "2022-08-31T02:27:02.631732Z",
          "shell.execute_reply.started": "2022-08-31T02:27:02.627445Z"
        },
        "id": "nSTivH_FSSf2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 40\n",
        "VOCABULARY_SIZE = 15000\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 512\n",
        "UNITS = 512\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:03.045344Z",
          "iopub.status.busy": "2022-08-31T02:27:03.044553Z",
          "iopub.status.idle": "2022-08-31T02:27:10.012465Z",
          "shell.execute_reply": "2022-08-31T02:27:10.011424Z",
          "shell.execute_reply.started": "2022-08-31T02:27:03.045304Z"
        },
        "id": "X8MGUNtBN2sz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    standardize=None,\n",
        "    output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "tokenizer.adapt(captions['caption'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:10.014991Z",
          "iopub.status.busy": "2022-08-31T02:27:10.014625Z",
          "iopub.status.idle": "2022-08-31T02:27:10.023341Z",
          "shell.execute_reply": "2022-08-31T02:27:10.022210Z",
          "shell.execute_reply.started": "2022-08-31T02:27:10.014950Z"
        },
        "id": "uafnas9Vw8be",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:10.025783Z",
          "iopub.status.busy": "2022-08-31T02:27:10.025328Z",
          "iopub.status.idle": "2022-08-31T02:27:10.117785Z",
          "shell.execute_reply": "2022-08-31T02:27:10.116791Z",
          "shell.execute_reply.started": "2022-08-31T02:27:10.025747Z"
        },
        "id": "Eqrd8hY0w8bf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:10.463258Z",
          "iopub.status.busy": "2022-08-31T02:27:10.462636Z",
          "iopub.status.idle": "2022-08-31T02:27:10.703505Z",
          "shell.execute_reply": "2022-08-31T02:27:10.702586Z",
          "shell.execute_reply.started": "2022-08-31T02:27:10.463221Z"
        },
        "id": "qvhg-6eKN3nz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:39.662780Z",
          "iopub.status.busy": "2022-08-31T02:27:39.662408Z",
          "iopub.status.idle": "2022-08-31T02:27:39.843396Z",
          "shell.execute_reply": "2022-08-31T02:27:39.842402Z",
          "shell.execute_reply.started": "2022-08-31T02:27:39.662750Z"
        },
        "id": "Yrca2aN2N5WL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(captions['image'], captions['caption']):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index], \n",
        "                                          img_keys[slice_index:])\n",
        "\n",
        "train_imgs = []\n",
        "train_captions = []\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    train_imgs.extend([imgt] * capt_len)\n",
        "    train_captions.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "val_imgs = []\n",
        "val_captions = []\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])\n",
        "    val_imgs.extend([imgv] * capv_len)\n",
        "    val_captions.extend(img_to_cap_vector[imgv])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:39.869167Z",
          "iopub.status.busy": "2022-08-31T02:27:39.868850Z",
          "iopub.status.idle": "2022-08-31T02:27:39.879342Z",
          "shell.execute_reply": "2022-08-31T02:27:39.878347Z",
          "shell.execute_reply.started": "2022-08-31T02:27:39.869140Z"
        },
        "id": "UHN3Q1YDN5TD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:50.927588Z",
          "iopub.status.busy": "2022-08-31T02:27:50.926697Z",
          "iopub.status.idle": "2022-08-31T02:27:50.934368Z",
          "shell.execute_reply": "2022-08-31T02:27:50.933286Z",
          "shell.execute_reply.started": "2022-08-31T02:27:50.927538Z"
        },
        "id": "12c-7FHzOFSq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    caption = tokenizer(caption)\n",
        "    return img, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:27:54.186327Z",
          "iopub.status.busy": "2022-08-31T02:27:54.185660Z",
          "iopub.status.idle": "2022-08-31T02:27:54.813746Z",
          "shell.execute_reply": "2022-08-31T02:27:54.812800Z",
          "shell.execute_reply.started": "2022-08-31T02:27:54.186289Z"
        },
        "id": "vHk83y3eOFPz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_imgs, train_captions))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_imgs, val_captions))\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:00.681202Z",
          "iopub.status.busy": "2022-08-31T02:28:00.680510Z",
          "iopub.status.idle": "2022-08-31T02:28:00.699433Z",
          "shell.execute_reply": "2022-08-31T02:28:00.698526Z",
          "shell.execute_reply.started": "2022-08-31T02:28:00.681163Z"
        },
        "id": "bQr_bgk11eMF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.2),\n",
        "        tf.keras.layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:02.691718Z",
          "iopub.status.busy": "2022-08-31T02:28:02.691183Z",
          "iopub.status.idle": "2022-08-31T02:28:02.699127Z",
          "shell.execute_reply": "2022-08-31T02:28:02.697943Z",
          "shell.execute_reply.started": "2022-08-31T02:28:02.691683Z"
        },
        "id": "H9GDJ9_1nIMO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def CNN_Encoder():\n",
        "    inception_v3 = tf.keras.applications.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    output = inception_v3.output\n",
        "    output = tf.keras.layers.Reshape(\n",
        "        (-1, output.shape[-1]))(output)\n",
        "\n",
        "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
        "    return cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:02.923161Z",
          "iopub.status.busy": "2022-08-31T02:28:02.922346Z",
          "iopub.status.idle": "2022-08-31T02:28:02.932114Z",
          "shell.execute_reply": "2022-08-31T02:28:02.931153Z",
          "shell.execute_reply.started": "2022-08-31T02:28:02.923119Z"
        },
        "id": "jMy5MrE2PdHV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
        "    \n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.layer_norm_1(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        attn_output = self.attention(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_2(x + attn_output)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:03.112672Z",
          "iopub.status.busy": "2022-08-31T02:28:03.111986Z",
          "iopub.status.idle": "2022-08-31T02:28:03.124208Z",
          "shell.execute_reply": "2022-08-31T02:28:03.123386Z",
          "shell.execute_reply.started": "2022-08-31T02:28:03.112636Z"
        },
        "id": "MFqNFts0duGB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Embeddings(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim)\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            max_len, embed_dim, input_shape=(None, max_len))\n",
        "    \n",
        "\n",
        "    def call(self, input_ids):\n",
        "        length = tf.shape(input_ids)[-1]\n",
        "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
        "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
        "\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        return token_embeddings + position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:03.614452Z",
          "iopub.status.busy": "2022-08-31T02:28:03.614096Z",
          "iopub.status.idle": "2022-08-31T02:28:03.630306Z",
          "shell.execute_reply": "2022-08-31T02:28:03.629269Z",
          "shell.execute_reply.started": "2022-08-31T02:28:03.614420Z"
        },
        "id": "pcbCQqrDnJ4-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = Embeddings(\n",
        "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
        "\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
        "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
        "    \n",
        "\n",
        "    def call(self, input_ids, encoder_output, training, mask=None):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        combined_mask = None\n",
        "        padding_mask = None\n",
        "        \n",
        "        if mask is not None:\n",
        "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attn_output_1 = self.attention_1(\n",
        "            query=embeddings,\n",
        "            value=embeddings,\n",
        "            key=embeddings,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
        "\n",
        "        attn_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_output,\n",
        "            key=encoder_output,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:04.053450Z",
          "iopub.status.busy": "2022-08-31T02:28:04.052714Z",
          "iopub.status.idle": "2022-08-31T02:28:04.070957Z",
          "shell.execute_reply": "2022-08-31T02:28:04.069996Z",
          "shell.execute_reply.started": "2022-08-31T02:28:04.053415Z"
        },
        "id": "9_NmSUaVys9R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_aug = image_aug\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "    \n",
        "\n",
        "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
        "        encoder_output = self.encoder(img_embed, training=True)\n",
        "        y_input = captions[:, :-1]\n",
        "        y_true = captions[:, 1:]\n",
        "        mask = (y_true != 0)\n",
        "        y_pred = self.decoder(\n",
        "            y_input, encoder_output, training=True, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
        "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    \n",
        "    def train_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        if self.image_aug:\n",
        "            imgs = self.image_aug(imgs)\n",
        "        \n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed, captions\n",
        "            )\n",
        "    \n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        )\n",
        "        grads = tape.gradient(loss, train_vars)\n",
        "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "    \n",
        "\n",
        "    def test_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        loss, acc = self.compute_loss_and_acc(\n",
        "            img_embed, captions, training=False\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:05.110311Z",
          "iopub.status.busy": "2022-08-31T02:28:05.107915Z",
          "iopub.status.idle": "2022-08-31T02:28:07.706765Z",
          "shell.execute_reply": "2022-08-31T02:28:07.705826Z",
          "shell.execute_reply.started": "2022-08-31T02:28:05.110262Z"
        },
        "id": "GqWpcsje0Hkh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
        "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
        "\n",
        "cnn_model = CNN_Encoder()\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:36.976255Z",
          "iopub.status.busy": "2022-08-31T02:28:36.975569Z",
          "iopub.status.idle": "2022-08-31T02:28:36.992494Z",
          "shell.execute_reply": "2022-08-31T02:28:36.991490Z",
          "shell.execute_reply.started": "2022-08-31T02:28:36.976216Z"
        },
        "id": "bayNssgNX6QN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "caption_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=cross_entropy\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T02:28:40.954232Z",
          "iopub.status.busy": "2022-08-31T02:28:40.953870Z",
          "iopub.status.idle": "2022-08-31T03:31:29.409591Z",
          "shell.execute_reply": "2022-08-31T03:31:29.407440Z",
          "shell.execute_reply.started": "2022-08-31T02:28:40.954202Z"
        },
        "id": "1RYo-MRVYn49",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T03:54:06.465688Z",
          "iopub.status.busy": "2022-08-31T03:54:06.465058Z",
          "iopub.status.idle": "2022-08-31T03:54:06.476806Z",
          "shell.execute_reply": "2022-08-31T03:54:06.475657Z",
          "shell.execute_reply.started": "2022-08-31T03:54:06.465646Z"
        },
        "id": "3ErlQQICtj_g",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_image_from_path(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T03:55:00.781733Z",
          "iopub.status.busy": "2022-08-31T03:55:00.780775Z",
          "iopub.status.idle": "2022-08-31T03:55:01.343341Z",
          "shell.execute_reply": "2022-08-31T03:55:01.342505Z",
          "shell.execute_reply.started": "2022-08-31T03:55:00.781682Z"
        },
        "id": "27_bJe_M1Drr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T04:00:00.835412Z",
          "iopub.status.busy": "2022-08-31T04:00:00.834851Z",
          "iopub.status.idle": "2022-08-31T04:00:04.067790Z",
          "shell.execute_reply": "2022-08-31T04:00:04.066774Z",
          "shell.execute_reply.started": "2022-08-31T04:00:00.835370Z"
        },
        "id": "YV4nrRA4w8by",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img_url = \"https://images.squarespace-cdn.com/content/v1/5e0e65adcd39ed279a0402fd/1627422658456-7QKPXTNQ34W2OMBTESCJ/1.jpg?format=2500w\"\n",
        "\n",
        "im = Image.open(requests.get(img_url, stream=True).raw)\n",
        "im = im.convert('RGB')\n",
        "im.save('tmp.jpg')\n",
        "\n",
        "pred_caption = generate_caption('tmp.jpg', add_noise=False)\n",
        "print('the image is of :', pred_caption)\n",
        "print()\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T03:45:38.511096Z",
          "iopub.status.busy": "2022-08-31T03:45:38.510481Z",
          "iopub.status.idle": "2022-08-31T03:45:39.056296Z",
          "shell.execute_reply": "2022-08-31T03:45:39.055297Z",
          "shell.execute_reply.started": "2022-08-31T03:45:38.511053Z"
        },
        "id": "9xts5jcCw8b0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "caption_model.save_weights('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG69m29gs6W4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_caption(img_path, add_noise=False):\n",
        "    img = load_image_from_path(img_path)\n",
        "    \n",
        "    if add_noise:\n",
        "        noise = tf.random.normal(img.shape)*0.1\n",
        "        img = img + noise\n",
        "        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n",
        "    \n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img_embed = caption_model.cnn_model(img)\n",
        "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
        "\n",
        "    y_inp = '[start]'\n",
        "    for i in range(MAX_LENGTH-1):\n",
        "        tokenized = tokenizer([y_inp])[:, :-1]\n",
        "        mask = tf.cast(tokenized != 0, tf.int32)\n",
        "        pred = caption_model.decoder(\n",
        "            tokenized, img_encoded, training=False, mask=mask)\n",
        "        \n",
        "        pred_idx = np.argmax(pred[0, i, :])\n",
        "        pred_idx = tf.convert_to_tensor(pred_idx)\n",
        "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
        "        if pred_word == '[end]':\n",
        "            break\n",
        "        \n",
        "        y_inp += ' ' + pred_word\n",
        "    \n",
        "    y_inp = y_inp.replace('[start] ', '')\n",
        "    return y_inp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio_client in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.3)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: fsspec in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (2023.1.0)\n",
            "Requirement already satisfied: httpx in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (0.13.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (22.0)\n",
            "Requirement already satisfied: requests in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (4.4.0)\n",
            "Requirement already satisfied: websockets in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio_client) (10.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio_client) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio_client) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio_client) (6.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (2022.12.7)\n",
            "Requirement already satisfied: hstspreload in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (2023.1.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (1.3.0)\n",
            "Requirement already satisfied: chardet==3.* in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->gradio_client) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==0.9.*->httpx->gradio_client) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==0.9.*->httpx->gradio_client) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx->gradio_client) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx->gradio_client) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gradio_client) (3.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\diya shajith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gradio_client) (1.26.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\diya shajith\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->gradio_client) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "date_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"https://suryabbrj-collegeprojectv2.hf.space/\")\n",
        "result = client.predict(\n",
        "\t\t\t\"https://images.pexels.com/photos/16314579/pexels-photo-16314579.jpeg?auto=compress&cs=tinysrgb&w=600&lazy=load\",api_name=\"/predict\"\n",
        ")\n",
        "\n",
        "with open(\"log.txt\", \"a\") as f:\n",
        "        f.write(\"\\n\" + date_string + \" : \\t\" + result + \"\\n\")\n",
        "\n",
        "\n",
        "print(result[0:-79])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
